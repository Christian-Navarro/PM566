---
title: "Week 7: Scraping, APIs, and Regular Expressions"
subtitle: "PM 566: Introduction to Health Data Science"
author: "George G. Vega Yon"
output:
  xaringan::moon_reader:
    css: ["theme.css", "default"]
    lib_dir: libs
    nature:
      beforeInit: "macros.js"
      ratio: "16:9"
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      titleSlideClass: [center, middle]
    # md_extensions:
    #   - -tex_math_single_backslash
    # keep_md: true
    mathjax: null
---

## Today's goals

- Introduction to Regular Expressions

- Understand the fundamentals of Web Scrapping

- Learn how to use an API

---

## Regular Expressions: What is it?

> A regular expression (shortened as regex or regexp; also referred to as rational expression) is a sequence of characters that define a search pattern. -- [Wikipedia](https://en.wikipedia.org/wiki/Regular_expression)

<img src="https://imgs.xkcd.com/comics/regular_expressions.png" width="450px">

---

## Regular Expressions: Why should you care?

We can use Regular Expressions for:

- Validating data fields, email address, numbers, etc.

- Searching text in various formats, e.g., addresses, there are many ways to write an address.

- Replace text, e.g., correcting spelling, `uhman|hiuman|uman` to `human`

- Remove text, e.g., tags from an HTML text, `<name>George</name>` to `George`.

---

## Regular Expressions 101: Type of fields

Most common:

- `.` Any character except new line
- `[0-9]` Numbers (without decimals or points)
- `[a-z]` Lower-case letters
- `[A-Z]` Upper-case letters
- `[a-zA-Z]` Lower or upper case letters.
- `[a-zA-Z0-9]` Any alpha-numeric
- `[^0-9]` any except numbers in the range 0-9
- `[^./ ]` any except dot, slash, and space.

Other important characters

- `\s` white space, equivalent to `[\r\n\t\f\v ]`
- `^` beginning of the text
- `$` end of the text
- `|` or (logical or)

---

## Regular Expressions 101: Type of fields (cont. 1)

These usually come together with specifying how many times (repetition):

- `regex?` Zero or one match.
- `regex*` Zero or more matches
- `regex+` One or more matches
- `regex{n,}` At least `n` matches
- `regex{,m}` at most `m` matches
- `regex{n,m}` Between `n` and `m` matches.

Where `regex` is a regular expression

---

## Regular Expressions 101: Type of fields (cont. 2)

There are other operators that can be very useful,

- `(regex)` Group capture.
- `(?:regex)` Group operation without capture.
- `(?=regex)` Look ahead (match)
- `(?!regex)` Look ahead (don't match)
- `(?<=regex)` Look behind (match)
- `(?<!regex)` Look behind (don't match)

Group captures can be reused with `\1`, `\2`, ...

More (great) information here https://regex101.com/


---

## Regular Expressions 101: Examples 

Here we are extracting the first occurrence of the following regular expressions
(using `stringr::str_extract()`):

```{r regex-examples-extract, echo = FALSE, results='asis'}
txt <- c("Hanna Perez [name]", "The year was 1999", "HaHa, @abc said that", "GoGo trojans #2020!")
regex <- c(
  ".{5}",
  "n{2}",
  "[0-9]+",
  "\\s[a-zA-Z]+\\s",
  "[a-zA-Z]+ [a-zA-Z]+", 
  "([a-zA-Z]+\\s?){2}",
  "([a-zA-Z]+)\\1",
  "(@|#)[a-z0-9]+",
  "(?:@|#)[a-z0-9]+",
  "(?<=#|@)[a-z0-9]+",
  "\\[[a-z]+\\]"
  )
ans <- lapply(regex, stringr::str_extract, string = txt)
ans <- do.call(rbind, ans)
colnames(ans) <- txt
ans <- cbind(regex = regex, ans)
ans[is.na(ans)] <- ""
ans[nrow(ans),1] <- "&#92&#91&#91a-z&#93+&#92&#93"
knitr::kable(ans, format = "html", escape = FALSE)
```

---
## Regular Expressions 101: Examples (cont. 1)

1. `r regex[1]` Match **any character** (except line end) five times.

2. `r regex[2]` Match the letter **n** twice.

3. `r regex[3]` Match **any number** at least once

4. `r regex[4]` Match a **space**, **any lower or upper case letter** at least once, and a **space**. 

5. `r regex[5]` Match two sets of letters separated by one space.

6. `r regex[6]` Match **any lower or upper case letter** at least once, maybe followed by a white space, twice.

7. `r regex[7]` Match **any lower or upper case letter** at least once, and then match the same pattern again.

8. `r regex[8]` Match either the `@` or `#` symbol, followed by one or more **lower case letter** or **number**.

9. `r regex[9]` Match either the `@` or `#` symbol, without capturing the group, followed by one or more **lower case letter** or **number**.

10. `r regex[10]` Match one or more **lower case letter** or **number** that follows either the `@` or `#` symbol.

11. \\ [[a-z]+\\ ] Match the symbol `[`, at least one **lower case letter**, and the symbol `]`.


---

## Regular Expressions 101: Functions in R

1. Lookup text: `base::grepl()`, `stringr::str_detect()`.

2. Find position: `base::grep()`, `stringr::str_which()`

3. Replace the first instance: `base::sub()`, `stringr::str_replace()`

4. Replace all instances: `base::gsub()`, `stringr::str_replace_all()`

5. Extract text: `base::regmatches()`, `stringr::str_extract()` and `stringr::str_extract_all()`.

---

## Data

This week we will continue using Textmining dataset (together with the `data.table` and `stringr` packages)

```{r load-data}
library(data.table)
library(stringr)

fn <- "mtsamples.csv"
if (!file.exists(fn))
  download.file(
    url = "https://github.com/USCbiostats/data-science-data/raw/master/00_mtsamples/mtsamples.csv",
    destfile = fn
  )
mtsamples <- fread(fn, sep = ",", header = TRUE)
```


---

## Regex Lookup Text: Tumor

We would like to see if this is tumor related entry. For that we can simply use
the following code:

```{r lookup-tumor}
# How many entries contain the word tumor
mtsamples[grepl("tumor", description, ignore.case = TRUE), .N] 

# Generating a column tagging tumor
mtsamples[, tumor_related := grepl("tumor", description, ignore.case = TRUE)]

# Taking a look at a few examples
mtsamples[tumor_related == TRUE, .(description)][1:3,]
```

Notice the `ignore.case = TRUE`. This is equivalent to transforming the text to lower case using `tolower()` before passing the text to the regular expression function.

---

## Regex Lookup text: Gender of the patient

Now, let's try to guess the gender of the patient. To do so, we could tag by
using the words *he, she, him, her, his, hers* (see [this article on sexist text](https://dictionary.cambridge.org/grammar/british-grammar/sexist-language?q=He%2C+she%2C+him%2C+her%2C+his%2C+hers)):

```{r}
mtsamples[, gender := str_extract(
  string  = tolower(transcription),
  pattern = "he|his|him|she|hers|her"
)]
```

What is the problem with this approach?

---

## Regex Lookup text: Gender of the patient (cont. 1)

For this we use the following regular expression:

`(?<=\W|^)(he|his|him|she|hers|her)(?=\W|$)`

- `(?<=regex)` lookback search.
- `\W` any non-word character, this is equivalent to `[^a-zA-Z0-9_]`, `|` or
- `^` the begging of text,
- `he|his|him...` any of these words,
- `(?=regex)` followed by,
- `\W` any non-word character, this is equivalent to `[^a-zA-Z0-9_]`, `|` or
- `$` the end of the text.

```{r gender-regex}
# Lower case everything
mtsamples[, gender := tolower(transcription)]
mtsamples[, gender := str_extract(
  string  = gender, 
  pattern = "(?<=\\W|^)(he|his|him|she|hers|her)(?=\\W|$)"
  )]
mtsamples[1:10, gender]
```

---

## Regex Lookup text: Gender of the patient (cont. 2)

```{r gender-regex-cont}
mtsamples[, female := fifelse(gender %in% c("he", "his", "him"), FALSE, TRUE)]
mtsamples[, table(gender, female, useNA = "always")]
```


---

## Regex Extract Text: Type of Cancer

- Imagine now that you need to see the types of Cancer mentioned in the data.

- For simplicity, let's assume that, if specified, it is in the form of `TYPE cancer`, i.e. single word.

- We are interested in the word before cancer, how can we capture this?

---

## Regex Extract Text: Type of Cancer (cont 1.)

We can just try to **extract** the phrase `"[some word] cancer"`, in particular, we could use the
following regular expression

`[a-zA-Z0-9-_]{4,}\s*cancer`

Where

- `[a-zA-Z0-9-_]{4,}` captures any character in the ranges `a-zA-Z0-9` including `-` and `_`. 
   Furthermore, for this match to work there must be at least 4 characters,
- `\s*` captures 0 or more white-spaces, and
- `cancer` captures the word cancer:

```{r cancer-regex}
mtsamples[, cancer_type := str_extract(tolower(keywords), "[a-zA-Z0-9-_]{4,}\\s*cancer")]
mtsamples[, table(cancer_type)]
```


---

## Fundamentals of Web Scrapping: What

> Web scraping, web harvesting, or web data extraction is data scraping used for extracting data from websites -- [Wikipedia](https://en.wikipedia.org/wiki/Web_scraping)


---

## Fundamentals of Web Scrapping: How

- Direct data download (download a plain-text-file) (already done this with MET, so we will skip)

- Raw HTML data (somewhat unstructured)

- Use an Application Programming Interface (API) (structured data)

---

## Fundamentals of Web Scrapping: How (cont.)

- The [`rvest`](https://cran.r-project.org/package=rvest) R package provides various tools for reading and processing web data.

- Under-the-hood, `rvest` is a wrapper of the [`xml2`](https://cran.r-project.org/package=xml2)
and [`httr`](https://cran.r-project.org/package=httr) R packages.

---

## Web scraping raw HTML: Example

We would like to capture the table of COVID-19 death rates per country directly from Wikipedia.

```{r setup-scrape}
library(rvest) # This loads the xml2 R package

# Reading the HTML table with the function xml2::read_html
covid <- read_html(
  x = "https://en.wikipedia.org/wiki/COVID-19_pandemic_death_rates_by_country"
  )

# Let's the the output
covid
```

---

## Web scraping raw HTML: Example (cont 1.)

- We want to get the HTML table that shows up in the doc To do such, we can use the
  function `xml2::xml_find_all()` and `rvest::html_table()`

- The first will locate the place in the document that matches a given **XPath**
  expression.
  
- [XPath](https://en.wikipedia.org/wiki/XPath), XML Path Language, is a query language to select nodes in a XML
  document.
  
- A nice tutorial can be found [here](https://www.w3schools.com/xml/xpath_intro.asp)

- Modern Web browsers make it easy to use XPath!

Live Example! (inspect elements in [Google Chrome](https://developers.google.com/web/tools/chrome-devtools/open),
[Mozilla Firefox](https://developer.mozilla.org/en-US/docs/Tools/Page_Inspector/How_to/Open_the_Inspector), [Internet Explorer](https://docs.microsoft.com/en-us/microsoft-edge/devtools-guide-chromium/ie-mode), and [Safari](https://developer.apple.com/library/archive/documentation/NetworkingInternetWeb/Conceptual/Web_Inspector_Tutorial/EditingCode/EditingCode.html#//apple_ref/doc/uid/TP40017576-CH4-DontLinkElementID_25))
  
---

## Web scraping with `xml2` and the `rvest` package (cont. 2)

Now that we know what is the path, let's use that and extract

```{r get-table-covid}
table <- xml_find_all(covid, xpath = "/html/body/div[3]/div[3]/div[5]/div[1]/table[2]")
table <- html_table(table) # This returns a list of tables
head(table[[1]])
```

---

## Web Scraping with APIs

For this part, we will be using the `httr()` package, which is a wrapper of the
`curl()` package, which in turn provides access to the `curl` library that
is used to communicate with APIs.

```{r setup-api}
library(httr)
```

---

## API Example 1: Using a Token

- Sometimes, APIs are not completely open, you need to register.

- In this example, I'm using a token which I obtained [here](https://www.ncdc.noaa.gov/cdo-web/token)

- You can find information about the [National Centers for Environmental Information](https://www.ncdc.noaa.gov/)
  API [here](https://www.ncdc.noaa.gov/cdo-web/webservices/v2)

---

## API Example 1: Using a Token (cont. 1)

- The way to pass the token will depend on the API service.

- Some require authentication, others need you to pass it as an argument of the query,
  i.e., directly in the URL.
  
- In this case, we pass it on the header.

```{r donwload-climate-data, eval = FALSE}
stations_api <- GET(
  url    = "https://www.ncdc.noaa.gov/cdo-web/api/v2/stations",
  config = add_headers(
    token = "UOPHhvGGZBOcoAeVWxTIjDImbzal"
    ),
  query  = list(limit = 1000)
)
```


This is equivalent to using the following query

```bash
curl --header "token: [YOURTOKEN]" \
  https://www.ncdc.noaa.gov/cdo-web/api/v2/stations?limit=1000
```

---

Again, we can recover the data using the `content()` function:

```{r download-climate-data-cont, eval = FALSE}
ans <- content(stations_api)
ans$results[[1]]
## $elevation
## [1] 139
## 
## $mindate
## [1] "1948-01-01"
## 
## $maxdate
## [1] "2014-01-01"
## 
## $latitude
## [1] 31.5702
## 
## $name
## [1] "ABBEVILLE, AL US"
## 
## $datacoverage
## [1] 0.8813
## 
## $id
## [1] "COOP:010008"
```

---

## Detour on CURL options

Sometimes you will need to change the default set of options in CURL. You can
checkout the list of options in `curl::curl_options()`. A common hack is to 
extend the time-limit before dropping the conection, e.g.:

Using the **Health IT** API from the US government, we can obtain the
**Electronic Prescribing Adoption and Use by County** (see docs
[here](https://dashboard.healthit.gov/datadashboard/documentation/electronic-prescribing-adoption-use-data-documentation-county.php))

The problem is that it usually takes longer to get the data, so we pass 
the config option `connecttimeout` (which corresponds to the flag `--connect-timeout`)
in the curl call (see next slide)

---

## Detour on CURL options (cont.)

```{r eval = FALSE}
ans <- httr::GET(
  url    = "https://dashboard.healthit.gov/api/open-api.php",
  query  = list(
    source = "AHA_2008-2015.csv",
    region = "California",
    period = 2015
    ),
  config = config(
    connecttimeout = 60
    )
)
```

```r
> ans$request
# <request>
# GET https://dashboard.healthit.gov/api/open-api.php?source=AHA_2008-2015.csv&region=California&period=2015
# Output: write_memory
# Options:
# * useragent: libcurl/7.58.0 r-curl/4.3 httr/1.4.1
# * connecttimeout: 60
# * httpget: TRUE
# Headers:
# * Accept: application/json, text/xml, application/xml, */*
```

---

## Regular Expressions: Email validation

This is the official regex for email validation implemented by [RCF 5322](http://www.ietf.org/rfc/rfc5322.txt) 

```
(?:[a-z0-9!#$%&'*+/=?^_`{|}~-]+(?:\.[a-z0-9!#$%&'*+/=?^_`{|}~-]+)*|"(?:[\x01-\x08
\x0b\x0c\x0e-\x1f\x21\x23-\x5b\x5d-\x7f]|\\[\x01-\x09\x0b\x0c\x0e-\x7f])*")@(?:(?
:[a-z0-9](?:[a-z0-9-]*[a-z0-9])?\.)+[a-z0-9](?:[a-z0-9-]*[a-z0-9])?|\[(?:(?:(2(5[
0-5]|[0-4][0-9])|1[0-9][0-9]|[1-9]?[0-9]))\.){3}(?:(2(5[0-5]|[0-4][0-9])|1[0-9][0
-9]|[1-9]?[0-9])|[a-z0-9-]*[a-z0-9]:(?:[\x01-\x08\x0b\x0c\x0e-\x1f\x21-\x5a\x53-\
x7f]|\\[\x01-\x09\x0b\x0c\x0e-\x7f])+)\])
```

See the corresponding post in [StackOverflow](https://stackoverflow.com/a/201378/2097171) 


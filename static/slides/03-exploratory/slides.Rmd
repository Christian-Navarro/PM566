---
title: "Exploratory Data Analysis"
subtitle: "PM 566: Introduction to Health Data Science"
author: "Meredith Franklin"
output:
  xaringan::moon_reader:
    css: ["theme.css"]
    lib_dir: libs
    nature:
      beforeInit: "macros.js"
      ratio: "16:9"
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      titleSlideClass: [center, middle]
---

```{r include=FALSE}
library(knitr)
hook_output <- knit_hooks$get("output")
knit_hooks$set(output = function(x, options) {
  lines <- options$output.lines
  if (is.null(lines)) {
    return(hook_output(x, options))  # pass to default hook
  }
  x <- unlist(strsplit(x, "\n"))
  more <- "..."
  if (length(lines) == 1) {        # first n lines
    if (length(x) > lines) {
      # truncate the output, but add ....
      x <- c(head(x, lines), more)
    }
  } else {
    x <- c(more, x[lines], more)
  }
  # paste these lines together
  x <- paste(c(x, ""), collapse = "\n")
  hook_output(x, options)
})
knit_hooks$set(output = function(x, options) {
  # this hook is used only when the linewidth option is not NULL
  if (!is.null(n <- options$linewidth)) {
    x = knitr:::split_lines(x)
    # any lines wider than n should be wrapped
    if (any(nchar(x) > n)) x = strwrap(x, width = n)
    x = paste(x, collapse = '\n')
  }
  hook_output(x, options)
})

opts_chunk$set(
  echo = TRUE,
  fig.width = 7, 
  fig.align = 'center',
  fig.asp = 0.618, # 1 / phi
  out.width = "700px")
```

```{r, echo = FALSE}
library(sass)
sass(sass_file("theme.sass"), output = "theme.css")
```

# Exploratory Data Analysis

* Exploratory data analysis is the process of summarizing data
* It should be the first step in your analysis pipeline
* It involves:
  + checking data (import issues, outliers, missing values, data errors)
  + cleaning data
  + summary statistics of key variables (univariate and bivariate)
  + basic plots and graphs

---
# Pipeline
.center[
![](img/data-science.png)
]

EDA involves Import -> Tidy -> Transform -> Visualize. Basically it is everything before we do modeling, prediction or inference.
---
# EDA Checklist

The goal of EDA is to better understand your data. Let's use the **checklist**:

  1. Formulate a question
  2. Read in the data
  3. Check the dimensions and headers and footers of the data
  4. Check the variable types in the data
  5. Take a closer look at some/all of the variables
  6. Validate with an external source
  7. Conduct some summary statistics to answer the initial question
  8. Make exploratory graphs

---
# Case study

We are going to use a dataset created from the National Center for Environmental Information (https://www.ncei.noaa.gov/). The data are 2019 hourly measurements from weather stations across the continental U.S.
---

# Formulate a Question

* It is a good idea to first have a question such as:
--

  + what weather stations reported the hottest and coldest temperatures? 
  + what are the summary statistics of the continuous variables in my dataset (mean, variance)?
  + is there covariation between two variables in my dataset?

---

# Read in the Data

There are several ways to read in data (some depend on the type of data you have):

* `read.table` or `read.csv` in base R for delimited files
* `readRDS` if you have a .rds dataset
* `read_csv`, `read_csv2`, `read_delim`, `read_fwf` from `library(readr)` that is part of the tidyverse
* `readxl()` from `library(readxl)` for .xls and .xlsx files
* `read_sas`, `read_spss`, `read_stata` from `library(haven)`
* `fread` from `library(data.table)` for efficiently importing large datasets that are regular delimited files
---

# Read in the Data

We will focus on base R, `data.table` and the `tidyverse`. Let's load the libraries we need to read in the data:
```{r, message=FALSE}
library(data.table)
library(tidyverse)
```
Let's load in the data. I have it stored locally, but we will see how to load it straight from github in the lab.
```{r}
met = data.table::fread("/Users/meredith/Dropbox (University of Southern California)/Courses/PM566/met_all.gz")
```
---

# Check the data

We should check the dimensions of the data set. This can be done several ways:
```{r}
dim(met)
nrow(met)
ncol(met)
```

We see that there are 2,377,343 records of hourly temperature in August 2019 from all of the weather stations in the US. The data set has 30 variables. 
---

# Check variables
```{r}
str(met)
```
---
# Check variables more closely

---

# Validate against an external source

---

# Summary statistics

---
#Exploratory Graphs

* With exploratory graphs we aim to:

--

  + debug any issues remaining in the data
  + understand properties of the data
  + look for patterns in the data
  + inform modeling strategies
  
--- 
# Exploratory Graphs
.center[
![](img/weather_map.png)
]

---
title: "Exploratory Data Analysis"
subtitle: "PM 566: Introduction to Health Data Science"
author: "Meredith Franklin"
output:
  xaringan::moon_reader:
    css: ["theme.css"]
    lib_dir: libs
    nature:
      beforeInit: "macros.js"
      ratio: "16:9"
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      titleSlideClass: [center, middle]
---

```{r include=FALSE}
library(knitr)
hook_output <- knit_hooks$get("output")
knit_hooks$set(output = function(x, options) {
  lines <- options$output.lines
  if (is.null(lines)) {
    return(hook_output(x, options))  # pass to default hook
  }
  x <- unlist(strsplit(x, "\n"))
  more <- "..."
  if (length(lines) == 1) {        # first n lines
    if (length(x) > lines) {
      # truncate the output, but add ....
      x <- c(head(x, lines), more)
    }
  } else {
    x <- c(more, x[lines], more)
  }
  # paste these lines together
  x <- paste(c(x, ""), collapse = "\n")
  hook_output(x, options)
})
knit_hooks$set(output = function(x, options) {
  # this hook is used only when the linewidth option is not NULL
  if (!is.null(n <- options$linewidth)) {
    x = knitr:::split_lines(x)
    # any lines wider than n should be wrapped
    if (any(nchar(x) > n)) x = strwrap(x, width = n)
    x = paste(x, collapse = '\n')
  }
  hook_output(x, options)
})

opts_chunk$set(
  echo = TRUE,
  fig.width = 7, 
  fig.align = 'center',
  fig.asp = 0.618, # 1 / phi
  out.width = "700px")
```

```{r, echo = FALSE}
library(sass)
sass(sass_file("theme.sass"), output = "theme.css")
```

# Exploratory Data Analysis

* Exploratory data analysis is the process of summarizing data
* It should be the first step in your analysis pipeline
* It involves:
--

  + checking data (import issues, outliers, missing values, data errors)
  + cleaning data
  + summary statistics of key variables (univariate and bivariate)
  + basic plots and graphs

---
# Pipeline
.center[
![](img/data-science.png)
]

EDA involves Import -> Tidy -> Transform -> Visualize. Basically it is everything before we do modeling, prediction or inference.

---
# EDA Checklist

The goal of EDA is to better understand your data. Let's use the **checklist**:
--

1.  Formulate a question
--

2.  Read in the data
--

3.  Check the dimensions and headers and footers of the data
--

4.  Check the variable types in the data
--

5.  Take a closer look at some/all of the variables
--

6.  Validate with an external source
--

7.  Conduct some summary statistics to answer the initial question
--

8.  Make exploratory graphs

---
# Case study

We are going to use a dataset created from the National Center for Environmental Information (https://www.ncei.noaa.gov/). The data are 2019 hourly measurements from weather stations across the continental U.S.

.center[
![:scale 60%](img/weather.png)
]

---

# Formulate a Question

- It is a good idea to first have a question such as:
--

  * what weather stations reported the hottest and coldest temperatures? 
  * what are the summary statistics of the continuous variables in my dataset (mean, variance)?
  * is there covariation between two variables in my dataset?

---

# Read in the Data

There are several ways to read in data (some depend on the type of data you have):

* `read.table` or `read.csv` in base R for delimited files
* `readRDS` if you have a .rds dataset
* `read_csv`, `read_csv2`, `read_delim`, `read_fwf` from `library(readr)` that is part of the tidyverse
* `readxl()` from `library(readxl)` for .xls and .xlsx files
* `read_sas`, `read_spss`, `read_stata` from `library(haven)`
* `fread` from `library(data.table)` for efficiently importing large datasets that are regular delimited files
---

# Read in the Data

We will focus on base R, `data.table` and the `tidyverse`. Let's load the libraries we need to read in the data:
```{r, message=FALSE}
library(data.table)
library(tidyverse)
```
Let's load in the data with `data.table`. I have it stored locally, but we will see how to load it straight from github in the lab.
```{r}
met = data.table::fread("/Users/meredith/Dropbox (University of Southern California)/Courses/PM566/met_all.gz")
```
---

# Read in the Data

`data.table` is a more efficient version of base R's `data.frame`. We create a `data.table` from an external data source by reading it in using `fread()`

We can convert existing `data.frame` or `list` objects to `data.table` by using `setDT()`

The nice thing about `data.table` is that it handles large datasets efficiently and it never reads/converts character type variables to factors, which can be a pain with `data.frame`

The other nice thing about `data.table` is that many of the base R functions work just as if it was a `data.frame`

---

# Check the data

We should check the dimensions of the data set. This can be done several ways:
```{r}
dim(met)
nrow(met)
ncol(met)
```


---

# Check the data

* We see that there are 2,377,343 records of hourly temperature in August 2019 from all of the weather stations in the US. The data set has 30 variables. 
* We should also check the top and bottom of the dataset to make sure that it imported correctly. Use `head(met)` and `tail(met)` for this.
* Next we can take a deeper dive into the contents of the data with `str()`
---


# Check variables
```{r}
str(met)
```
---


# Check variables

* First, we see that `str()` gives us the class of the data, which in this case is both `data.table` and `data.frame`, as well as the dimensions of the data
* We also see the variable names and their type (integer, numeric, character) 
* We can identify major problems with the data at this stage (e.g. a variable that has all missing values)

--

We can get summary statistics on our `data.table` using `summary()` as we would with a regular `data.frame`

---

# Check variables

```{r, out.height="50%", out.width="50%"}
summary(met[,8:13])
```
---

# Check variables more closely

We know that we are supposed to have hourly measurements of weather data for the month of August 2019 for the entire US. We should check that we have all of these components. Let us check:

* the year
* the month
* the hours
* the range of locations (latitude and longitude)

---

# Check variables more closely

We can generate tables for integer values
```{r}
table(met$hour)
table(met$month)
```
---

# Check variables more closely

For numeric values we should do a summary to see the quantiles, max, min

```{r}
table(met$year)
summary(met$lat)
summary(met$lon)
```
---

# Check variables more closely

Let's check some of the weather variables
```{r}
summary(met$temp)
```
It looks like the temperatures are in Celcius. A temperature of -40 in August is really cold, we should see if this is an implausiple value.

It also looks like there are a lot of missing data. Let us check the proportion of missings

```{r}
dim(met[is.na(temp)])[1]/dim(met)[1]
```

---



# Check variables more closely

In `data.table` we can easily subset the data and select certain columns
```{r}
met_ss = met[temp == -40.00, .(hour, lat, lon, elev, wind.sp)]

dim(met_ss)
summary(met_ss)
```
---

# Check variables more closely

In `dplyr` we can do the same thing using  `filter` 
```{r}
met_ss = filter(met, temp == -40.00) %>% 
        select(USAFID, day, hour, 
               lat, lon, elev, wind.sp)

dim(met_ss)
summary(met_ss)
```

---

# Validate against an external source

We should check various things to make sure that our data make sense, for example the observation with -40C is suspicious, so we should look up the location of the weather station.

Go to [Google maps](https://www.google.com/maps/) and enter the coordinates for the site with -40C (29.12, -89.55)

.center[
![:scale 30%](img/map.png)
]
---

# Summary statistics

---
# Exploratory Graphs

* With exploratory graphs we aim to:

--

  + debug any issues remaining in the data
  + understand properties of the data
  + look for patterns in the data
  + inform modeling strategies
  
--- 
# Exploratory Graphs
.center[
![](img/weather_map.png)
]

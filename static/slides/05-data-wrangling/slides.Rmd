---
title: "High performance computing, cloud computing "
subtitle: "PM 566: Introduction to Health Data Science"
author: "George G. Vega Yon"
output:
  # slidy_presentation
  xaringan::moon_reader:
    css: ["theme.css"]
    lib_dir: libs
    nature:
      beforeInit: "macros.js"
      ratio: "16:9"
      highlightStyle: github
      highlightLines: false
      countIncrementalSlides: false
      titleSlideClass: [center, middle]
  #   code_download: true
---

<!--Yeah... I have really long code chunks, so I just changed the default size :)-->
<style type="text/css">
code.r,code.cpp,code{
  font-size:medium;
}
</style>

# Part I: intro

---

## Today's goals

We will learn about how to manipulate data, and in particular, creating/modifying variables
taking the following into consideration:

- Groups
- Shifted
- Date/time types
- Factor variables
- Text variables
- Presence of missings

We will want to create summaries of the data.

---

## Scenarios

1. The 9, 99, 999, ... coding for missigness.

2. The data contains dates

3. The data is a panel dataset with various measures per unit, and we need to create a lag

4. Need to create some summaries at the group level

5. Have two datasets and need to merge them by one or more keys/indices/ids.

6. Create a running mean spanning the last 7 observations.

---

## Data Management: data.table

The data.table package:

- Is very fast
- Is memory efficient
- Is 

---

## Benchmarks

- [H2O.ai](https://www.h2o.ai/)'s benchmark ([link](https://h2oai.github.io/db-benchmark/)): Designed by
  the lead developer of data.table [Matt Dowle](https://rdatatable.gitlab.io/data.table/)

- [RStudio's](https://rstudio.com) benchmark ([link](https://cran.r-project.org/web/packages/vroom/vignettes/benchmarks.html)):
  Designed as part of the benchmarks with the [vroom](https://cran.r-package.org/package=vroom) package.

---

# Part II: The basics

---

## Packages for the session

For this session we will be using the following two R packages:

```{r loading}
library(dtplyr)
library(data.table)
```

The `dtplyr` R package translates `dplyr` (`tidyverse`) syntax to `data.table`, so that we can still use **the pipe** while at the same time leveraging the performance of `data.table`.


---

## Loading the data

The data that we will be using is an already processed version of the MET dataset. We can download (and load) the data directly in our session using the following commands

```{r downloading-data, cache=TRUE}
# Where are we getting the data from
met_url <- "https://github.com/USCbiostats/data-science-data/raw/master/02_met/met_all.gz"

# Downloading the data to a tempfile (so it is destroyed afterwards)
# you can replace this with, for example, your own data:
tmp <- tempfile(fileext = ".gz")

if (!file.exists(tmp)) {
  download.file(
    url      = met_url,
    destfile = tmp,
    # method   = "libcurl", timeout = 1000 (you may need this option)
  )
}

dat <- fread(tmp)
```

---

Let's take a look at the data using the `head()` function

```{r}
head(dat)
```

Now that we have the data, we can start working with it!

---

## Filtering (subsetting) the data

Need to select records according to some criteria. For example:

```{r select}

```


---

## Generating summary statistics

```{r read-in, cache=TRUE}
library(data.table)

# Generating a date component
dat[, Date := as.IDate(paste(year, month, day, sep = "-"))]

# Generating the daily averages of Atmospheric pressure and temperature per
# station
dat_daily <- dat[
  ,
  list(
    atm.press_avg = mean(atm.press, na.rm = TRUE),
    temp_avg      = mean(temp, na.rm = TRUE)
  ),
  by = c("USAFID", "Date", "lat", "lon")
]
```


---

```{r}
# Monthly average temperature plot
library(ggplot2)
library(akima)
dat_daily_plot <- with(
  dat_daily[mday(Date) == 1 & is.finite(temp_avg)],
  akima::interp(
    x = lon, y = lat, z = temp_avg
  )
)

image(dat_daily_plot)
```

---

```{r, eval = FALSE}
library(dtplyr)
library(dplyr)
data(diamonds, package="ggplot2")
dt_tibble <- lazy_dt(diamonds)
dt_dt     <- lazy_dt(as.data.table(diamonds))
dt        <- as.data.table(diamonds)
microbenchmark::microbenchmark(
  dplyr         = diamonds %>% group_by(carat) %>% summarise(mean(price), .groups="drop") %>% collect(),
  dtplyr_tibble = dt_tibble %>% group_by(carat) %>% summarise(mean(price)) %>% collect(),
  dtplyr_dt     = dt_dt %>% group_by(carat) %>% summarise(mean(price), .groups="drop") %>% collect(),
  data.table    = dt[, mean(price), by = carat]
)

```

```{r, eval = FALSE}
tmp <- tempfile(fileext="csv")
download.file(
  "https://raw.githubusercontent.com/Rdatatable/data.table/master/vignettes/flights14.csv",
  destfile = tmp,
  method   = "libcurl",
  timeout  = 1000
  )
dat_dt <- fread(tmp)
dat_tb <- as_tibble(dat_dt)
dat_lz <- lazy_dt(dat_tb)
```


```{r, eval = FALSE}
microbenchmark::microbenchmark(
  dplyr         = dat_tb %>% group_by(dest) %>% summarise(mean(air_time), .groups="drop") %>% collect(),
  dtplyr_tibble = dat_lz %>% group_by(dest) %>% summarise(mean(air_time), .groups="drop") %>% collect(),
  data.table    = dat_dt[, mean(air_time), by = dest], times = 100
)
```



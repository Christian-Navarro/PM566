<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Text Mining</title>
    <meta charset="utf-8" />
    <meta name="author" content="Emil Hvitfeldt" />
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link rel="stylesheet" href="theme.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, title-slide

# Text Mining
## PM566 - Week 7
### Emil Hvitfeldt

---






# Plan for the week

- We will try to turn text into numbers
- Then use tidy principals to explore those numbers

---

# Tidytext

![](images/tidytext.png)

---

# Why tidytext?

Works seemlessly with ggplot2, dplyr and tidyr.

**Alternatives:**

**R**: quanteda, tm, koRpus

**Python**: nltk, Spacy, gensim

---

# Alice's Adventures in Wonderland


```r
#devtools::install_github("EmilHvitfeldt/tidygutenbergr")
library(tidyverse)
```

```
## ── Attaching packages ─────────────────────────────────────────────── tidyverse 1.3.0 ──
```

```
## ✓ tibble 3.0.3     ✓ purrr  0.3.4
## ✓ tidyr  1.1.0
```

```
## ── Conflicts ────────────────────────────────────────────────── tidyverse_conflicts() ──
## x dplyr::filter() masks stats::filter()
## x dplyr::lag()    masks stats::lag()
```

```r
library(tidygutenbergr)

alice &lt;- alices_adventures()
```

---

# Turning the data into a tidy format


```r
alice
```

```
## # A tibble: 3,330 x 3
##    text                                                       chapter chapter_name      
##    &lt;chr&gt;                                                        &lt;int&gt; &lt;chr&gt;             
##  1 "CHAPTER I. Down the Rabbit-Hole"                                1 Down the Rabbit-H…
##  2 ""                                                               1 Down the Rabbit-H…
##  3 "Alice was beginning to get very tired of sitting by her …       1 Down the Rabbit-H…
##  4 "bank, and of having nothing to do: once or twice she had…       1 Down the Rabbit-H…
##  5 "book her sister was reading, but it had no pictures or c…       1 Down the Rabbit-H…
##  6 "it, 'and what is the use of a book,' thought Alice 'with…       1 Down the Rabbit-H…
##  7 "conversations?'"                                                1 Down the Rabbit-H…
##  8 ""                                                               1 Down the Rabbit-H…
##  9 "So she was considering in her own mind (as well as she c…       1 Down the Rabbit-H…
## 10 "hot day made her feel very sleepy and stupid), whether t…       1 Down the Rabbit-H…
## # … with 3,320 more rows
```

---

# Turning the data into a tidy format


```r
library(tidytext)
alice %&gt;%
  unnest_tokens(token, text)
```

```
## # A tibble: 26,683 x 3
##    chapter chapter_name         token    
##      &lt;int&gt; &lt;chr&gt;                &lt;chr&gt;    
##  1       1 Down the Rabbit-Hole chapter  
##  2       1 Down the Rabbit-Hole i        
##  3       1 Down the Rabbit-Hole down     
##  4       1 Down the Rabbit-Hole the      
##  5       1 Down the Rabbit-Hole rabbit   
##  6       1 Down the Rabbit-Hole hole     
##  7       1 Down the Rabbit-Hole alice    
##  8       1 Down the Rabbit-Hole was      
##  9       1 Down the Rabbit-Hole beginning
## 10       1 Down the Rabbit-Hole to       
## # … with 26,673 more rows
```

---

# Words as a unit

Now that we have words as the observation unit we can use the **dplyr** toolbox.

---

# Using dplyr verbs

.pull-left[

```r
library(dplyr)
alice %&gt;%
  unnest_tokens(token, text)
```
]

.pull-right[

```
## # A tibble: 26,683 x 3
##    chapter chapter_name         token    
##      &lt;int&gt; &lt;chr&gt;                &lt;chr&gt;    
##  1       1 Down the Rabbit-Hole chapter  
##  2       1 Down the Rabbit-Hole i        
##  3       1 Down the Rabbit-Hole down     
##  4       1 Down the Rabbit-Hole the      
##  5       1 Down the Rabbit-Hole rabbit   
##  6       1 Down the Rabbit-Hole hole     
##  7       1 Down the Rabbit-Hole alice    
##  8       1 Down the Rabbit-Hole was      
##  9       1 Down the Rabbit-Hole beginning
## 10       1 Down the Rabbit-Hole to       
## # … with 26,673 more rows
```
]

---

# Using dplyr verbs

.pull-left[

```r
library(dplyr)
alice %&gt;%
  unnest_tokens(token, text) %&gt;%
  count(token)
```
]

.pull-right[

```
## # A tibble: 2,628 x 2
##    token          n
##    &lt;chr&gt;      &lt;int&gt;
##  1 _i_            2
##  2 a            632
##  3 abide          1
##  4 able           1
##  5 about         94
##  6 above          3
##  7 absence        1
##  8 absurd         2
##  9 acceptance     1
## 10 accident       2
## # … with 2,618 more rows
```
]

---

# Using dplyr verbs

.pull-left[

```r
library(dplyr)
alice %&gt;%
  unnest_tokens(token, text) %&gt;%
  count(token, sort = TRUE)
```
]

.pull-right[

```
## # A tibble: 2,628 x 2
##    token     n
##    &lt;chr&gt; &lt;int&gt;
##  1 the    1643
##  2 and     872
##  3 to      729
##  4 a       632
##  5 she     541
##  6 it      530
##  7 of      514
##  8 said    462
##  9 i       408
## 10 alice   386
## # … with 2,618 more rows
```
]

---

# Using dplyr verbs

.pull-left[

```r
library(dplyr)
alice %&gt;%
  unnest_tokens(token, text) %&gt;%
  count(chapter, token)
```
]

.pull-right[

```
## # A tibble: 7,370 x 3
##    chapter token          n
##      &lt;int&gt; &lt;chr&gt;      &lt;int&gt;
##  1       1 a             52
##  2       1 about          8
##  3       1 across         2
##  4       1 actually       1
##  5       1 advice         1
##  6       1 advise         1
##  7       1 afraid         1
##  8       1 after          5
##  9       1 afterwards     1
## 10       1 again          4
## # … with 7,360 more rows
```
]

---

# Using dplyr verbs

.pull-left[

```r
library(dplyr)
alice %&gt;%
  unnest_tokens(token, text) %&gt;%
  group_by(chapter) %&gt;%
  count(token) %&gt;%
  top_n(10, n)
```
]

.pull-right[

```
## # A tibble: 121 x 3
## # Groups:   chapter [12]
##    chapter token     n
##      &lt;int&gt; &lt;chr&gt; &lt;int&gt;
##  1       1 a        52
##  2       1 alice    27
##  3       1 and      65
##  4       1 i        30
##  5       1 it       62
##  6       1 of       43
##  7       1 she      79
##  8       1 the      92
##  9       1 to       75
## 10       1 was      53
## # … with 111 more rows
```
]

---

# Using dplyr verbs and ggplot2

.pull-left[

```r
library(dplyr)
library(ggplot2)
alice %&gt;%
  unnest_tokens(token, text) %&gt;%
  count(token) %&gt;%
  top_n(10, n) %&gt;%
  ggplot(aes(n, token)) +
  geom_col()
```
]

.pull-right[
&lt;img src="slides_files/figure-html/unnamed-chunk-11-1.png" width="700px" style="display: block; margin: auto;" /&gt;
]

---

# Using dplyr verbs and ggplot2

.pull-left[

```r
library(dplyr)
library(ggplot2)
library(forcats)
alice %&gt;%
  unnest_tokens(token, text) %&gt;%
  count(token) %&gt;%
  top_n(10, n) %&gt;%
  ggplot(aes(n, fct_reorder(token, n))) +
  geom_col()
```
]

.pull-right[
&lt;img src="slides_files/figure-html/unnamed-chunk-12-1.png" width="700px" style="display: block; margin: auto;" /&gt;
]

---

### Medical Transcriptions

Loading in reference transcription samples from https://www.mtsamples.com/


```r
library(readr)
mt_samples &lt;- read_csv("mtsamples.csv")
mt_samples &lt;- mt_samples %&gt;%
  select(description, medical_specialty, transcription)

head(mt_samples)
```

```
## # A tibble: 6 x 3
##   description                      medical_specialty   transcription                    
##   &lt;chr&gt;                            &lt;chr&gt;               &lt;chr&gt;                            
## 1 A 23-year-old white female pres… Allergy / Immunolo… "SUBJECTIVE:,  This 23-year-old …
## 2 Consult for laparoscopic gastri… Bariatrics          "PAST MEDICAL HISTORY:, He has d…
## 3 Consult for laparoscopic gastri… Bariatrics          "HISTORY OF PRESENT ILLNESS: , I…
## 4 2-D M-Mode. Doppler.             Cardiovascular / P… "2-D M-MODE: , ,1.  Left atrial …
## 5 2-D Echocardiogram               Cardiovascular / P… "1.  The left ventricular cavity…
## 6 Morbid obesity.  Laparoscopic a… Bariatrics          "PREOPERATIVE DIAGNOSIS: , Morbi…
```

---

## What specialties do we have?




```r
mt_samples %&gt;%
  count(medical_specialty)
```

```
## # A tibble: 40 x 2
##    medical_specialty              n
##    &lt;chr&gt;                      &lt;int&gt;
##  1 Allergy / Immunology           7
##  2 Autopsy                        8
##  3 Bariatrics                    18
##  4 Cardiovascular / Pulmonary   372
##  5 Chiropractic                  14
##  6 Consult - History and Phy.   516
##  7 Cosmetic / Plastic Surgery    27
##  8 Dentistry                     27
##  9 Dermatology                   29
## 10 Diets and Nutritions          10
## # … with 30 more rows
```

---

## What specialties do we have?


```r
mt_samples %&gt;%
  count(medical_specialty, sort = TRUE)
```

```
## # A tibble: 40 x 2
##    medical_specialty                 n
##    &lt;chr&gt;                         &lt;int&gt;
##  1 Surgery                        1103
##  2 Consult - History and Phy.      516
##  3 Cardiovascular / Pulmonary      372
##  4 Orthopedic                      355
##  5 Radiology                       273
##  6 General Medicine                259
##  7 Gastroenterology                230
##  8 Neurology                       223
##  9 SOAP / Chart / Progress Notes   166
## 10 Obstetrics / Gynecology         160
## # … with 30 more rows
```


---

## Sample transprict


```
## SUBJECTIVE:, This 23-year-old white female presents with complaint of allergies.
## She used to have allergies when she lived in Seattle but she thinks they are
## worse here. In the past, she has tried Claritin, and Zyrtec. Both worked for
## short time but then seemed to lose effectiveness. She has used Allegra also.
## She used that last summer and she began using it again two weeks ago. It does
## not appear to be working very well. She has used over-the-counter sprays but
## no prescription nasal sprays. She does have asthma but doest not require daily
## medication for this and does not think it is flaring up.,MEDICATIONS: , Her only
## medication currently is Ortho Tri-Cyclen and the Allegra.,ALLERGIES: , She has
## no known medicine allergies.,OBJECTIVE:,Vitals: Weight was 130 pounds and blood
## pressure 124/78.,HEENT: Her throat was mildly erythematous without exudate.
## Nasal mucosa was erythematous and swollen. Only clear drainage was seen. TMs
## were clear.,Neck: Supple without adenopathy.,Lungs: Clear.,ASSESSMENT:, Allergic
## rhinitis.,PLAN:,1. She will try Zyrtec instead of Allegra again. Another option
## will be to use loratadine. She does not think she has prescription coverage so
## that might be cheaper.,2. Samples of Nasonex two sprays in each nostril given
## for three weeks. A prescription was written as well.
```

---

# Your turn 1

- Tokenize the the words in the `transcription` column
- Count the number of times each token appears
- Visualize the top 20 most frequent words

---

# Your turn 1 - Solutions


```
## # A tibble: 23,647 x 2
##    word         n
##    &lt;chr&gt;    &lt;int&gt;
##  1 the     149888
##  2 and      82779
##  3 was      71765
##  4 of       59205
##  5 to       50632
##  6 a        42810
##  7 with     35815
##  8 in       32807
##  9 is       26378
## 10 patient  22065
## # … with 23,637 more rows
```

---

# Your turn 1 - Solutions

&lt;img src="slides_files/figure-html/unnamed-chunk-18-1.png" width="700px" style="display: block; margin: auto;" /&gt;

---

A lot of the words don't tell us very much. Words such as "the", "and", "at" and "for" appear a lot in English text but doens't add much to the context.

Words such as these are called **stop words**

For more information about differences in stop words and when to remove them read this chapter https://smltar.com/stopwords

---

## Stop words in tidytext

tidytext comes with a data.frame of stop words


```r
stop_words
```

```
## # A tibble: 1,149 x 2
##    word        lexicon
##    &lt;chr&gt;       &lt;chr&gt;  
##  1 a           SMART  
##  2 a's         SMART  
##  3 able        SMART  
##  4 about       SMART  
##  5 above       SMART  
##  6 according   SMART  
##  7 accordingly SMART  
##  8 across      SMART  
##  9 actually    SMART  
## 10 after       SMART  
## # … with 1,139 more rows
```


---

## Removing stopwords

We can use an `anti_join()` to remove the tokens that also appear in the `stop_words` data.frame


```r
mt_samples %&gt;%
  unnest_tokens(word, transcription) %&gt;%
  anti_join(stop_words, by = c("word")) %&gt;%
  count(word, sort = TRUE)
```

```
## # A tibble: 23,025 x 2
##    word          n
##    &lt;chr&gt;     &lt;int&gt;
##  1 patient   22065
##  2 left      11258
##  3 history    9509
##  4 2          8864
##  5 1          8396
##  6 normal     7526
##  7 procedure  7463
##  8 3          6093
##  9 pain       5976
## 10 4          5318
## # … with 23,015 more rows
```

---

## Your turn 2

- Redo visualization but remove stopwords before
- Bonus points if you remove numbers as well

---

## Your turn 2 - solution

&lt;img src="slides_files/figure-html/unnamed-chunk-21-1.png" width="700px" style="display: block; margin: auto;" /&gt;

---

## Which words appears together?

ngrams are n coonsecutive word, we can count these to see what words appears together.

- ngram with n = 1 are called unigrams: "which", "words", "appears", "together"
- ngram with n = 2 are called bigrams: "which words", "words appears", "appears together"
- ngram with n = 3 are called trigrams: "which words appears", "words appears together"

---

## Which words appears together?

We can extract bigrams using `unnest_ngrams()` with `n = 2`


```r
mt_samples %&gt;%
  unnest_ngrams(ngram, transcription, n = 2)
```

```
## # A tibble: 2,398,059 x 3
##    description                                    medical_specialty        ngram        
##    &lt;chr&gt;                                          &lt;chr&gt;                    &lt;chr&gt;        
##  1 1-month-old for a healthy checkup - Well chil… Consult - History and P… subjective t…
##  2 1-month-old for a healthy checkup - Well chil… Consult - History and P… this is      
##  3 1-month-old for a healthy checkup - Well chil… Consult - History and P… is a         
##  4 1-month-old for a healthy checkup - Well chil… Consult - History and P… a 1          
##  5 1-month-old for a healthy checkup - Well chil… Consult - History and P… 1 month      
##  6 1-month-old for a healthy checkup - Well chil… Consult - History and P… month old    
##  7 1-month-old for a healthy checkup - Well chil… Consult - History and P… old who      
##  8 1-month-old for a healthy checkup - Well chil… Consult - History and P… who comes    
##  9 1-month-old for a healthy checkup - Well chil… Consult - History and P… comes in     
## 10 1-month-old for a healthy checkup - Well chil… Consult - History and P… in for       
## # … with 2,398,049 more rows
```

---

# Which words appears together?

Tallying up the bi-grams still shows a lot of stop words but it able to pick up retationhips with patients


```r
mt_samples %&gt;%
  unnest_ngrams(ngram, transcription, n = 2) %&gt;%
  count(ngram, sort = TRUE)
```

```
## # A tibble: 301,399 x 2
##    ngram           n
##    &lt;chr&gt;       &lt;int&gt;
##  1 the patient 20301
##  2 of the      19050
##  3 in the      12784
##  4 to the      12372
##  5 was then     6952
##  6 and the      6346
##  7 patient was  6291
##  8 the right    5509
##  9 on the       5241
## 10 the left     4858
## # … with 301,389 more rows
```

---

# Which words appears together?


```r
mt_samples %&gt;%
  unnest_ngrams(ngram, transcription, n = 2) %&gt;%
  separate(ngram, into = c("word1", "word2"), sep = " ") %&gt;%
  select(word1, word2)
```

```
## # A tibble: 2,398,059 x 2
##    word1      word2
##    &lt;chr&gt;      &lt;chr&gt;
##  1 subjective this 
##  2 this       is   
##  3 is         a    
##  4 a          1    
##  5 1          month
##  6 month      old  
##  7 old        who  
##  8 who        comes
##  9 comes      in   
## 10 in         for  
## # … with 2,398,049 more rows
```

---

# Your turn 3 

- Pick a word and count the words that appears after and before it
- how does the result change if you look at trigrams

---

# Your turn 3 - solution

I picked the word "blood". These are the most common words to appear after


```
## # A tibble: 161 x 2
##    word2        n
##    &lt;chr&gt;    &lt;int&gt;
##  1 pressure  1265
##  2 loss       965
##  3 cell       130
##  4 in         114
##  5 cells      112
##  6 sugar       91
##  7 and         84
##  8 sugars      79
##  9 was         65
## 10 cultures    53
## # … with 151 more rows
```

---

# Your turn 3 - solution

These are the most common words to appear before


```
## # A tibble: 439 x 2
##    word1         n
##    &lt;chr&gt;     &lt;int&gt;
##  1 estimated   754
##  2 white       180
##  3 signs       170
##  4 and         154
##  5 of          149
##  6 red         123
##  7 her         116
##  8 his          99
##  9 the          96
## 10 no           72
## # … with 429 more rows
```

---

# Your turn 3 - solution

These are the most common pair of words to appear after "blood"


```
## # A tibble: 799 x 3
##    word2    word3       n
##    &lt;chr&gt;    &lt;chr&gt;   &lt;int&gt;
##  1 loss     minimal   176
##  2 pressure is        176
##  3 loss     less      162
##  4 loss     was       142
##  5 cell     count     119
##  6 pressure was        75
##  7 pressure of         70
##  8 pressure and        54
##  9 in       the        53
## 10 loss     none       43
## # … with 789 more rows
```

---

# Your turn 4

Find your own insight in the data:

Ideas:

- Interesting ngrams
- See if certain words are used more in some specialties then others
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script src="macros.js"></script>
<script>var slideshow = remark.create({
"ratio": "16:9",
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>

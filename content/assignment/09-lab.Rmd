---
title: "Lab 10 - HPC"
output: tufte::tufte_html
link-citations: yes
---

# Learning goals

In this lab, you are expected to learn/put in practice the following skills:

- Evaluate whether a problem can be parallelized or not.
- Practice with the parallel package.
- Practice your skills with Git.

## Problem 1: Think

Give yourself a few minutes to think about what you just learned. List three
examples of problems that you believe may be solved using parallel computing,
and check for packages on the HPC CRAN task view that may be related to it.

## Problem 2: Before you

The following functions can be written to be more efficient without using
parallel:

1. This function generates a `n x k` dataset with all its entries distributed
poission with mean `lambda`.

```r
fun1 <- function(n = 100, k = 4, lambda = 4) {
  x <- NULL
  for (i in 1:n)
    x <- rbind(x, rpois(k, lambda))
}
```

2.  Find the column max (hint: Checkout the function `max.col()`).

```r
# Data Generating Process (10 x 10,000 matrix)
set.seed(1234)
x <- matrix(rnorm(1e4), nrow=10)

# Find each column's max value
fun2 <- function(x) {
  apply(x, 2, max)
}
```

## Problem 3: Parallelize everyhing

We will now turn our attention to non-parametric 
[bootstrapping](https://en.wikipedia.org/wiki/Bootstrapping_(statistics)).
Among the many uses, non-parametric bootstrapping allow us to obtain confidence
intervals for parameter estimates without relying on parametric assumptions.

The main assumption is that we can approximate many experiments by resampling
observations from our original dataset. 

El método de Bootstrap (Efron 1979) (o remuestreo) puede ser utilizado para aproximar distribuciones de estadísticos de manera no-paramétrica–que significa sin asumir una forma funcional en particular–con el simple supuesto de que la muestra observada refleja la distribución de la población.

La siguiente función implementa bootstrap noparamétrico de manera secuencial (no en paralelo)

```
mi_boot <- function(dat, stat, R, ncpus = 1L) {
  
  # Generando indices aleatorios
  n <- nrow(dat)
  idx <- matrix(sample.int(n, n*R, TRUE), nrow=n, ncol=R)
 
  # Creando cluster 
  # ..... ACA VA EL CODIGO PARA ARMAR EL CLUSTER
  
  # Llamando funcion
  # ...... ESTA FUNCION DEBE SER REEMPLAZADA POR parSapply...
  t(sapply(seq_len(R), function(i) {
    stat(dat[idx[,i], , drop=FALSE])
  }))
  
}
# Bootstrap the modelo OLS
mi_stat <- function(d) coef(lm(y ~ x, data=d))

# Simulando data
set.seed(1)
n <- 500; R <- 1e4

x <- cbind(rnorm(n)); y <- x*5 + rnorm(n)

# Chequeando que obtenemos datos razonables. Boot puede ser comparado con el 
# intervalo de confianza que obtenemos de la regresion lineal.
ans0 <- confint(lm(y~x))
ans1 <- mi_boot(dat = data.frame(x, y), mi_stat, R = R, ncpus = 2L)

# Comparando IC con el bootstrap rustico y el modelo OLS.
t(apply(ans1, 2, quantile, c(.025,.975)))
##                   2.5%      97.5%
## (Intercept) -0.1372435 0.05074397
## x            4.8680977 5.04539763
ans0
##                  2.5 %     97.5 %
## (Intercept) -0.1379033 0.04797344
## x            4.8650100 5.04883353
```

Siguiendo los comentarios incluidos en el cuerpo de la función, modifica la función para que implemente bootstrapping en paralelo. Compara tus resultados utilizando 1, 2 y (si disponible) 4 procesadores.

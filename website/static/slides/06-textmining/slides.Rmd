---
title: "Text Mining"
subtitle: "PM566 - Week 6"
author: "Emil Hvitfeldt"
date: ""
output:
  xaringan::moon_reader:
    css: ["theme.css", "default"]
    lib_dir: libs
    nature:
      beforeInit: "macros.js"
      ratio: "16:9"
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      titleSlideClass: [center, middle]
---

```{r include=FALSE}
library(knitr)
hook_output <- knit_hooks$get("output")
knit_hooks$set(output = function(x, options) {
  lines <- options$output.lines
  if (is.null(lines)) {
    return(hook_output(x, options))  # pass to default hook
  }
  x <- unlist(strsplit(x, "\n"))
  more <- "..."
  if (length(lines) == 1) {        # first n lines
    if (length(x) > lines) {
      # truncate the output, but add ....
      x <- c(head(x, lines), more)
    }
  } else {
    x <- c(more, x[lines], more)
  }
  # paste these lines together
  x <- paste(c(x, ""), collapse = "\n")
  hook_output(x, options)
})
knit_hooks$set(output = function(x, options) {
  # this hook is used only when the linewidth option is not NULL
  if (!is.null(n <- options$linewidth)) {
    x = knitr:::split_lines(x)
    # any lines wider than n should be wrapped
    if (any(nchar(x) > n)) x = strwrap(x, width = n)
    x = paste(x, collapse = '\n')
  }
  hook_output(x, options)
})

opts_chunk$set(
  echo = TRUE,
  fig.width = 7, 
  fig.align = 'center',
  fig.asp = 0.618, # 1 / phi
  out.width = "700px")
```

```{r, echo = FALSE}
library(sass)
# library(countdown)
sass(sass_file("theme.sass"), output = "theme.css")
```

```{css, echo = FALSE}
.orange {color: #EF8633}
```

## Acknowledgment

These slides were originally developed by Emil Hvitfeldt and modified by George G. Vega Yon.

---

# Plan for the week

- We will try to turn text into numbers
- Then use tidy principals to explore those numbers

---

![](images/tidytext.png)

---

# Why tidytext?

Works seemlessly with ggplot2, dplyr and tidyr.

**Alternatives:**

**R**: quanteda, tm, koRpus

**Python**: nltk, Spacy, gensim

---

# Alice's Adventures in Wonderland

Download the alice dataset from https://USCBiostats.github.io/PM566/slides/06-textmining/alice.rds (or from [here](alice.rds))

```{r message=FALSE, warning=FALSE}
library(tidyverse)
alice <- readRDS("alice.rds")
alice
```

---

# Tokenizing

Turning text into smaller units

--

In English:

- split by spaces
- more advanced algorithms

---

# Spacy tokenizer

![](images/spacy.png)

---

# Turning the data into a tidy format

```{r}
library(tidytext)
alice %>%
  unnest_tokens(token, text)
```

---

# Words as a unit

Now that we have words as the observation unit we can use the **dplyr** toolbox.

---

# Using dplyr verbs

.pull-left[
```{r dplyr1, eval=FALSE}
library(dplyr)
alice %>%
  unnest_tokens(token, text)
```
]

.pull-right[
```{r ref.label="dplyr1", echo=FALSE}
```
]

---

# Using dplyr verbs

.pull-left[
```{r dplyr2, eval=FALSE}
library(dplyr)
alice %>%
  unnest_tokens(token, text) %>%
  count(token)
```
]

.pull-right[
```{r ref.label="dplyr2", echo=FALSE}
```
]

---

# Using dplyr verbs

.pull-left[
```{r dplyr3, eval=FALSE}
library(dplyr)
alice %>%
  unnest_tokens(token, text) %>%
  count(token, sort = TRUE)
```
]

.pull-right[
```{r ref.label="dplyr3", echo=FALSE}
```
]

---

# Using dplyr verbs

.pull-left[
```{r dplyr4, eval=FALSE}
library(dplyr)
alice %>%
  unnest_tokens(token, text) %>%
  count(chapter, token)
```
]

.pull-right[
```{r ref.label="dplyr4", echo=FALSE}
```
]

---

# Using dplyr verbs

.pull-left[
```{r dplyr5, eval=FALSE}
library(dplyr)
alice %>%
  unnest_tokens(token, text) %>%
  group_by(chapter) %>%
  count(token) %>%
  top_n(10, n)
```
]

.pull-right[
```{r ref.label="dplyr5", echo=FALSE}
```
]

---

# Using dplyr verbs and ggplot2

.pull-left[
```{r dplyr6, eval=FALSE}
library(dplyr)
library(ggplot2)
alice %>%
  unnest_tokens(token, text) %>%
  count(token) %>%
  top_n(10, n) %>%
  ggplot(aes(n, token)) +
  geom_col()
```
]

.pull-right[
```{r ref.label="dplyr6", echo=FALSE}
```
]

---

# Using dplyr verbs and ggplot2

.pull-left[
```{r dplyr7, eval=FALSE}
library(dplyr)
library(ggplot2)
library(forcats)
alice %>%
  unnest_tokens(token, text) %>%
  count(token) %>%
  top_n(10, n) %>%
  ggplot(aes(n, fct_reorder(token, n))) +
  geom_col()
```
]

.pull-right[
```{r ref.label="dplyr7", echo=FALSE}
```
]

---

# Stop words

A lot of the words don't tell us very much. Words such as "the", "and", "at" and "for" appear a lot in English text but doesn't add much to the context.

Words such as these are called **stop words**

For more information about differences in stop words and when to remove them read this chapter https://smltar.com/stopwords

---

## Stop words in tidytext

tidytext comes with a data.frame of stop words

```{r}
stop_words
```

---

# snowball stopwords

```{r stopwords1, echo=FALSE}
stop_words %>%
  filter(lexicon == "snowball") %>%
  pull(word)
```

---

# funky stop words quiz #1

.pull-left[
- he's
- she's
- himself
- herself
]


---

# funky stop words quiz #1

.pull-left[
- he's
- .orange[she's]
- himself
- herself
]

.pull-right[
.orange[she's] doesn't appear in the SMART list
]

---

# funky stop words quiz #2

.pull-left[
- owl
- bee
- fify
- system1
]


---

# funky stop words quiz #2

.pull-left[
- owl
- bee
- .orange[fify]
- system1
]

.pull-right[
.orange[fify] was left undetected for 3 years (2012 to 2015) in scikit-learn
]

---

# funky stop words quiz #3

.pull-left[
- substantially
- successfully
- sufficiently
- statistically
]


---

# funky stop words quiz #3

.pull-left[
- substantially
- successfully
- sufficiently
- .orange[statistically]
]

.pull-right[
.orange[statistically] doesn't appear in the Stopwords ISO list
]


---

## Removing stopwords

We can use an `anti_join()` to remove the tokens that also appear in the `stop_words` data.frame

.pull-left[
```{r stopwords2, eval=FALSE}
alice %>%
  unnest_tokens(token, text) %>%
  anti_join(stop_words, by = c("token" = "word")) %>%
  count(token, sort = TRUE)
```
]

.pull-right[
```{r ref.label="stopwords2", echo=FALSE}
```
]


---

## Anti-join with same variable name

.pull-left[
```{r stopwords3, eval=FALSE}
alice %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words, by = c("word")) %>%
  count(word, sort = TRUE)
```
]

.pull-right[
```{r ref.label="stopwords3", echo=FALSE}
```
]


---

# Stop words removed

.pull-left[
```{r stopwords4, eval=FALSE}
alice %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words, by = c("word")) %>%
  count(word, sort = TRUE) %>%
  top_n(10, n) %>%
  ggplot(aes(n, fct_reorder(word, n))) +
  geom_col()
```
]

.pull-right[
```{r ref.label="stopwords4", echo=FALSE}
```
]

---

## Which words appears together?

**ngrams** are n consecutive word, we can count these to see what words appears together.

--

- ngram with n = 1 are called unigrams: "which", "words", "appears", "together"
- ngram with n = 2 are called bigrams: "which words", "words appears", "appears together"
- ngram with n = 3 are called trigrams: "which words appears", "words appears together"

---

## Which words appears together?

We can extract bigrams using `unnest_ngrams()` with `n = 2`

```{r}
alice %>%
  unnest_ngrams(ngram, text, n = 2)
```

---

# Which words appears together?

Tallying up the bi-grams still shows a lot of stop words but it able to pick up retationhips with patients

```{r}
alice %>%
  unnest_ngrams(ngram, text, n = 2) %>%
  count(ngram, sort = TRUE)
```

---

# Which words appears together?

```{r}
alice %>%
  unnest_ngrams(ngram, text, n = 2) %>%
  separate(ngram, into = c("word1", "word2"), sep = " ") %>%
  select(word1, word2)
```

---

```{r}
alice %>%
  unnest_ngrams(ngram, text, n = 2) %>%
  separate(ngram, into = c("word1", "word2"), sep = " ") %>%
  select(word1, word2) %>%
  filter(word1 == "alice")
```

---

```{r}
alice %>%
  unnest_ngrams(ngram, text, n = 2) %>%
  separate(ngram, into = c("word1", "word2"), sep = " ") %>%
  select(word1, word2) %>%
  filter(word1 == "alice") %>%
  count(word2, sort = TRUE)
```

---

```{r}
alice %>%
  unnest_ngrams(ngram, text, n = 2) %>%
  separate(ngram, into = c("word1", "word2"), sep = " ") %>%
  select(word1, word2) %>%
  filter(word2 == "alice") %>%
  count(word1, sort = TRUE)
```

---

# TF-IDF

TF: Term frequency  
IDF: Inverse document frequency


TF-IDF: product of TF and IDF

TF gives weight to terms that appear a lot, IDF gives weight to terms that appears in a few documents

---

# TF-IDF with tidytext

.pull-left[
```{r tfidf1, eval=FALSE}
alice %>%
  unnest_tokens(text, text)
```
]

.pull-right[
```{r ref.label="tfidf1", echo=FALSE}
```
]

---

# TF-IDF with tidytext

.pull-left[
```{r tfidf2, eval=FALSE}
alice %>%
  unnest_tokens(text, text) %>%
  count(text, chapter)
```
]

.pull-right[
```{r ref.label="tfidf2", echo=FALSE}
```
]

---

# TF-IDF with tidytext

.pull-left[
```{r tfidf3, eval=FALSE}
alice %>%
  unnest_tokens(text, text) %>%
  count(text, chapter) %>%
  bind_tf_idf(text, chapter, n)
```
]

.pull-right[
```{r ref.label="tfidf3", echo=FALSE}
```
]

---

# TF-IDF with tidytext

.pull-left[
```{r tfidf4, eval=FALSE}
alice %>%
  unnest_tokens(text, text) %>%
  count(text, chapter) %>%
  bind_tf_idf(text, chapter, n) %>%
  arrange(desc(tf_idf))
```
]

.pull-right[
```{r ref.label="tfidf4", echo=FALSE}
```
]

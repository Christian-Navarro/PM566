---
title: "Text Mining"
subtitle: "PM566 - Week 6"
author: "Emil Hvitfeldt and Kelly Street"
date: ""
output:
  xaringan::moon_reader:
    css: ["theme.css", "default"]
    lib_dir: libs
    nature:
      beforeInit: "macros.js"
      ratio: "16:9"
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      titleSlideClass: [center, middle]
---

```{r include=FALSE}
library(knitr)
hook_output <- knit_hooks$get("output")
knit_hooks$set(output = function(x, options) {
  lines <- options$output.lines
  if (is.null(lines)) {
    return(hook_output(x, options))  # pass to default hook
  }
  x <- unlist(strsplit(x, "\n"))
  more <- "..."
  if (length(lines) == 1) {        # first n lines
    if (length(x) > lines) {
      # truncate the output, but add ....
      x <- c(head(x, lines), more)
    }
  } else {
    x <- c(more, x[lines], more)
  }
  # paste these lines together
  x <- paste(c(x, ""), collapse = "\n")
  hook_output(x, options)
})
knit_hooks$set(output = function(x, options) {
  # this hook is used only when the linewidth option is not NULL
  if (!is.null(n <- options$linewidth)) {
    x = knitr:::split_lines(x)
    # any lines wider than n should be wrapped
    if (any(nchar(x) > n)) x = strwrap(x, width = n)
    x = paste(x, collapse = '\n')
  }
  hook_output(x, options)
})

opts_chunk$set(
  echo = TRUE,
  fig.width = 7, 
  fig.align = 'center',
  fig.asp = 0.618, # 1 / phi
  out.width = "700px")
```

```{r, echo = FALSE}
library(sass)
# library(countdown)
sass(sass_file("theme.sass"), output = "theme.css")
```

```{css, echo = FALSE}
.orange {color: #EF8633}
```

## Acknowledgment

These slides were originally developed by Emil Hvitfeldt and modified by George G. Vega Yon.

---

# Plan for the week

- We will try to turn text into numbers
- Then use tidy principals to explore those numbers

---

![](images/tidytext.png)

---

# Why tidytext?

Works seemlessly with ggplot2, dplyr and tidyr.

**Alternatives:**

**R**: quanteda, tm, koRpus

**Python**: nltk, Spacy, gensim

---

# Alice's Adventures in Wonderland

Download the alice dataset from https://USCBiostats.github.io/PM566/slides/06-textmining/alice.rds (or from [here](alice.rds))

```{r message=FALSE, warning=FALSE}
library(tidyverse)
alice <- readRDS("alice.rds")
alice
```

---

# Tokenizing

Turning text into smaller units

--

In English:

- split by spaces
- more advanced algorithms

---

# Spacy tokenizer

![](images/spacy.png)

---

# Turning the data into a tidy format

```{r}
library(tidytext)
alice |>
  unnest_tokens(token, text)
```

---

# Words as a unit

Now that we have words as the observation unit we can use the **dplyr** toolbox.

---

# Using dplyr verbs

.pull-left[
```{r dplyr1, eval=FALSE}
library(dplyr)
alice |>
  unnest_tokens(token, text)
```
]

.pull-right[
```{r ref.label="dplyr1", echo=FALSE}
```
]

---

# Using dplyr verbs

.pull-left[
```{r dplyr2, eval=FALSE}
library(dplyr)
alice |>
  unnest_tokens(token, text) |>
  count(token)
```
]

.pull-right[
```{r ref.label="dplyr2", echo=FALSE}
```
]

---

# Using dplyr verbs

.pull-left[
```{r dplyr3, eval=FALSE}
library(dplyr)
alice |>
  unnest_tokens(token, text) |>
  count(token, sort = TRUE)
```
]

.pull-right[
```{r ref.label="dplyr3", echo=FALSE}
```
]

---

# Using dplyr verbs

.pull-left[
```{r dplyr4, eval=FALSE}
library(dplyr)
alice |>
  unnest_tokens(token, text) |>
  count(chapter, token)
```
]

.pull-right[
```{r ref.label="dplyr4", echo=FALSE}
```
]

---

# Using dplyr verbs

.pull-left[
```{r dplyr5, eval=FALSE}
library(dplyr)
alice |>
  unnest_tokens(token, text) |>
  group_by(chapter) |>
  count(token) |>
  top_n(10, n)
```
]

.pull-right[
```{r ref.label="dplyr5", echo=FALSE}
```
]

---

# Using dplyr verbs and ggplot2

.pull-left[
```{r dplyr6, eval=FALSE}
library(dplyr)
library(ggplot2)
alice |>
  unnest_tokens(token, text) |>
  count(token) |>
  top_n(10, n) |>
  ggplot(aes(n, token)) +
  geom_col()
```
]

.pull-right[
```{r ref.label="dplyr6", echo=FALSE}
```
]

---

# Using dplyr verbs and ggplot2

.pull-left[
```{r dplyr7, eval=FALSE}
library(dplyr)
library(ggplot2)
library(forcats)
alice |>
  unnest_tokens(token, text) |>
  count(token) |>
  top_n(10, n) |>
  ggplot(aes(n, fct_reorder(token, n))) +
  geom_col()
```
]

.pull-right[
```{r ref.label="dplyr7", echo=FALSE}
```
]

---

# Stop words

A lot of the words don't tell us very much. Words such as "the", "and", "at" and "for" appear a lot in English text but doesn't add much to the context.

Words such as these are called **stop words**

For more information about differences in stop words and when to remove them read this chapter https://smltar.com/stopwords

---

## Stop words in tidytext

tidytext comes with a data.frame of stop words

```{r}
stop_words
```

---

# snowball stopwords

```{r stopwords1, echo=FALSE}
stop_words |>
  filter(lexicon == "snowball") |>
  pull(word)
```

---

# funky stop words quiz #1

.pull-left[
- he's
- she's
- himself
- herself
]


---

# funky stop words quiz #1

.pull-left[
- he's
- .orange[she's]
- himself
- herself
]

.pull-right[
.orange[she's] doesn't appear in the SMART list
]

---

# funky stop words quiz #2

.pull-left[
- owl
- bee
- fify
- system1
]


---

# funky stop words quiz #2

.pull-left[
- owl
- bee
- .orange[fify]
- system1
]

.pull-right[
.orange[fify] was left undetected for 3 years (2012 to 2015) in scikit-learn
]

---

# funky stop words quiz #3

.pull-left[
- substantially
- successfully
- sufficiently
- statistically
]


---

# funky stop words quiz #3

.pull-left[
- substantially
- successfully
- sufficiently
- .orange[statistically]
]

.pull-right[
.orange[statistically] doesn't appear in the Stopwords ISO list
]


---

## Removing stopwords

We can use an `anti_join()` to remove the tokens that also appear in the `stop_words` data.frame

.pull-left[
```{r stopwords2, eval=FALSE}
alice |>
  unnest_tokens(token, text) |>
  anti_join(stop_words, by = c("token" = "word")) |>
  count(token, sort = TRUE)
```
]

.pull-right[
```{r ref.label="stopwords2", echo=FALSE}
```
]


---

## Anti-join with same variable name

.pull-left[
```{r stopwords3, eval=FALSE}
alice |>
  unnest_tokens(word, text) |>
  anti_join(stop_words, by = c("word")) |>
  count(word, sort = TRUE)
```
]

.pull-right[
```{r ref.label="stopwords3", echo=FALSE}
```
]


---

# Stop words removed

.pull-left[
```{r stopwords4, eval=FALSE}
alice |>
  unnest_tokens(word, text) |>
  anti_join(stop_words, by = c("word")) |>
  count(word, sort = TRUE) |>
  top_n(10, n) |>
  ggplot(aes(n, fct_reorder(word, n))) +
  geom_col()
```
]

.pull-right[
```{r ref.label="stopwords4", echo=FALSE}
```
]

---

## Which words appears together?

**ngrams** are n consecutive word, we can count these to see what words appears together.

--

- ngram with n = 1 are called unigrams: "which", "words", "appears", "together"
- ngram with n = 2 are called bigrams: "which words", "words appears", "appears together"
- ngram with n = 3 are called trigrams: "which words appears", "words appears together"

---

## Which words appears together?

We can extract bigrams using `unnest_ngrams()` with `n = 2`

```{r}
alice |>
  unnest_ngrams(ngram, text, n = 2)
```

---

# Which words appears together?

Tallying up the bi-grams still shows a lot of stop words but it able to pick up retationhips with patients

```{r}
alice |>
  unnest_ngrams(ngram, text, n = 2) |>
  count(ngram, sort = TRUE)
```

---

# Which words appears together?

```{r}
alice |>
  unnest_ngrams(ngram, text, n = 2) |>
  separate(ngram, into = c("word1", "word2"), sep = " ") |>
  select(word1, word2)
```

---

```{r}
alice |>
  unnest_ngrams(ngram, text, n = 2) |>
  separate(ngram, into = c("word1", "word2"), sep = " ") |>
  select(word1, word2) |>
  filter(word1 == "alice")
```

---

```{r}
alice |>
  unnest_ngrams(ngram, text, n = 2) |>
  separate(ngram, into = c("word1", "word2"), sep = " ") |>
  select(word1, word2) |>
  filter(word1 == "alice") |>
  count(word2, sort = TRUE)
```

---

```{r}
alice |>
  unnest_ngrams(ngram, text, n = 2) |>
  separate(ngram, into = c("word1", "word2"), sep = " ") |>
  select(word1, word2) |>
  filter(word2 == "alice") |>
  count(word1, sort = TRUE)
```

---

# TF-IDF

TF: Term frequency  
IDF: Inverse document frequency


TF-IDF: product of TF and IDF

TF gives weight to terms that appear a lot, IDF gives weight to terms that appears in a few documents

---

# TF-IDF with tidytext

.pull-left[
```{r tfidf1, eval=FALSE}
alice |>
  unnest_tokens(text, text)
```
]

.pull-right[
```{r ref.label="tfidf1", echo=FALSE}
```
]

---

# TF-IDF with tidytext

.pull-left[
```{r tfidf2, eval=FALSE}
alice |>
  unnest_tokens(text, text) |>
  count(text, chapter)
```
]

.pull-right[
```{r ref.label="tfidf2", echo=FALSE}
```
]

---

# TF-IDF with tidytext

.pull-left[
```{r tfidf3, eval=FALSE}
alice |>
  unnest_tokens(text, text) |>
  count(text, chapter) |>
  bind_tf_idf(text, chapter, n)
```
]

.pull-right[
```{r ref.label="tfidf3", echo=FALSE}
```
]

---

# TF-IDF with tidytext

.pull-left[
```{r tfidf4, eval=FALSE}
alice |>
  unnest_tokens(text, text) |>
  count(text, chapter) |>
  bind_tf_idf(text, chapter, n) |>
  arrange(desc(tf_idf))
```
]

.pull-right[
```{r ref.label="tfidf4", echo=FALSE}
```
]

---

# Sentiment Analysis

Also known as "opinion mining," sentiment analysis is a way in which we can use computers to attempt to understand the feelings conveyed by a piece of text. This generally relies on a large, human-compiled database of words with known associations such as "positive" and "negative" or specific feelings like "joy", "surprise", "disgust", etc.

---

# Sentiment Analysis

![](https://www.tidytextmining.com/images/tmwr_0201.png)


---

# Sentiment Lexicons

The `tidytext` and `textdata` packages provide access to three different databases of words and their associated sentiments (known as "sentiment lexicons"). Obviously, none of these can be perfect, as there is no "correct" way to quantify feelings, but they all attempt to capture different elements of how a text makes you feel.

The readily available lexicons are:
 - `afinn` from [Finn Årup Nielsen](http://www2.imm.dtu.dk/pubdb/views/publication_details.php?id=6010)
 - `bing` from [Bing Liu and collaborators](https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html)
 - `nrc` from [Saif Mohammad and Peter Turney](https://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm)

---

# Sentiment Lexicons - `bing`

The `bing` lexicon contains a large list of words and a binary association, either "positive" or "negative":

```{r}
get_sentiments('bing')
```

---

# Sentiment Lexicons - `afinn`

The `afinn` lexicon goes slightly further, assigning words a value between -5 and 5 that represents their positivity or negativity.

```{r}
get_sentiments('afinn')
```

---

# Sentiment Lexicons - `nrc`

The `nrc` lexicon takes a different approach and assigns each word an associated sentiment. Some words appear more than once because they have multiple associations:

```{r}
get_sentiments('nrc')
```

---

# Sentiment Analysis

We can use one of these databases to analyze _Alice's Adventures in Wonderland_ by breaking the text down into words and combining the result with a lexicon. Let's use `bing` to assign "positive" and "negative" labels to as many words as possible in the book. (Note that this time the variable created by `unnest_tokens` is called `word`, to match the column name in `bing`).

```{r}
alice |>
  unnest_tokens(word, text) |>
  inner_join(get_sentiments("bing"))
```

---

# Sentiment Analysis

We can now group and summarize this new dataset the same as any other. For example, let's look at the sentiment by chapter. We'll do this by counting the number of "positive" words and subtracting the number of "negative" words:

```{r}
diff_by_chap <- alice |>
  unnest_tokens(word, text) |>
  inner_join(get_sentiments("bing")) |> 
  group_by(chapter) |> 
  summarise(sentiment = sum(sentiment == "positive") - sum(sentiment == "negative"))
```


---

# Sentiment Analysis

```{r}
barplot(diff_by_chap$sentiment, names.arg = diff_by_chap$chapter)
```

---

# Sentiment Analysis

Alternatively, we could use the `afinn` lexicon and quantify the "sentiment" of each chapter by the average of all words with numeric associations:

```{r}
avg_by_chap <- alice |>
  unnest_tokens(word, text) |>
  inner_join(get_sentiments("afinn")) |> 
  group_by(chapter) |> 
  summarise(sentiment = mean(value))
```

---

# Sentiment Analysis

```{r}
barplot(avg_by_chap$sentiment, names.arg = avg_by_chap$chapter)
```

---

# Sentiment Analysis

Similarly, we can find the most frequent sentiment association in the `nrc` lexicon for each chapter. Unfortunately, for all chapters, the most frequent sentiment association ends up being the rather bland "positive" or "negative":

```{r, warning=FALSE}
alice |>
  unnest_tokens(word, text) |>
  inner_join(get_sentiments("nrc")) |> 
  group_by(chapter) |> 
  summarise(sentiment = names(which.max(table(sentiment))))
```

---

# Sentiment Analysis

We'll try to spice things up by removing "positive" and "negative" from the `nrc` lexicon:

```{r, warning=FALSE}
nrc_fun <- get_sentiments("nrc")
nrc_fun <- nrc_fun[!nrc_fun$sentiment %in% c("positive","negative"), ]
```

---

# Sentiment Analysis

Now we see a lot of "anticipation":

```{r, warning=FALSE}
alice |>
  unnest_tokens(word, text) |>
  inner_join(nrc_fun) |> 
  group_by(chapter) |> 
  summarise(sentiment = names(which.max(table(sentiment))))
```
